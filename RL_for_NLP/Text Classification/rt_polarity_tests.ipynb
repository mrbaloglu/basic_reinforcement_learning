{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning\")\n",
    "from RL_for_NLP.text_environments import TextEnvClfBert, TextEnvClf\n",
    "from RL_for_NLP.text_data_pools import PartialReadingDataPoolWithTokens, PartialReadingDataPoolWithBertTokens\n",
    "import NLP_utils.preprocessing as nlp_processing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning/NLP_datasets/RT_Polarity',\n",
       " 'max_len': 50,\n",
       " 'vocab_size': 28996}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning/NLP_datasets/RT_Polarity/data_info_bert.json\", \"r\") as f:\n",
    "    data_info = json.load(f)\n",
    "data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = nlp_processing.openDfFromPickle(data_info[\"path\"] + \"/rt-polarity-train-bert.pkl\")\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare some hyperparameters\n",
    "\n",
    "WINDOW_SIZE = 5\n",
    "MAX_STEPS = int(1e+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = nlp_processing.openDfFromPickle(data_info[\"path\"] + \"/rt-polarity-test-bert.pkl\")\n",
    "data_val = nlp_processing.openDfFromPickle(data_info[\"path\"] + \"/rt-polarity-val-bert.pkl\")\n",
    "\n",
    "train_pool = PartialReadingDataPoolWithBertTokens(data_train, \"review\", \"label\", \"good\", WINDOW_SIZE)\n",
    "test_pool = PartialReadingDataPoolWithBertTokens(data_test, \"review\", \"label\", \"good\", WINDOW_SIZE)\n",
    "val_pool = PartialReadingDataPoolWithBertTokens(data_val, \"review\", \"label\", \"good\", WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = TextEnvClfBert(train_pool, MAX_STEPS)\n",
    "val_env = TextEnvClfBert(val_pool, 1000)\n",
    "test_env = TextEnvClfBert(test_pool, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env.current_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import reinforce_algorithm_utils as rl_monte_carlo\n",
    "import gym\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_env.current_state_input_id.shape)\n",
    "s_size = train_env.current_state_input_id.shape[0]\n",
    "a_size = len(train_env.action_space)\n",
    "print(s_size, a_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"h_sizes\": [64, 32],\n",
    "    \"n_training_episodes\": 500,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 100,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": None,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "import policy_networks as pn\n",
    "\"\"\"policy = pn.Transformer_Baseline_Policy(data_info[\"vocab_size\"], hyperparameters[\"state_space\"], hyperparameters[\"action_space\"],\n",
    "                                        num_heads=1, num_layers=1)\"\"\"\n",
    "policy = pn.RNN_Baseline_Policy(data_info[\"vocab_size\"], hyperparameters[\"state_space\"], hyperparameters[\"action_space\"])\n",
    "optimizer = Adam(policy.parameters(), lr=hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rl_monte_carlo.reinforce_algorithm(train_env, policy,\n",
    "                   optimizer,\n",
    "                   hyperparameters[\"n_training_episodes\"], \n",
    "                   hyperparameters[\"max_t\"],\n",
    "                   hyperparameters[\"gamma\"], \n",
    "                   50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_agent(train_env, 10, 100, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_agent(val_env, 10, 100, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_on_clf(val_env, policy, pos_label=\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow.set_tracking_uri(\"file:/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning/agent_models/rt-polarity\")\n",
    "mlflow.set_experiment(\"rnn\")\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"n_episodes\", 500)\n",
    "mlflow.pytorch.log_model(policy, \"rnn-default\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning/agent_models/rt-polarity\")\n",
    "uri = \"runs:/9f09e3fa6df6427ba2889e89ac787bcc/rnn-default\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_agent(env, 10, 100, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_on_clf(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, env):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    actions = []\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        action = action.item()\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        actions.append(env.action_space.ix_to_action(action))\n",
    "        total_reward += rewards\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(f\"Predicted Label {actions}\")\n",
    "    print(f\"Oracle Label: {env.current_label}\")\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "    print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(policy=MlpPolicy, env=train_env, learning_rate=0.001, batch_size=3)\n",
    "\n",
    "for i in range(int(5)):\n",
    "    model.learn(total_timesteps=int(1e+3), reset_num_timesteps=False)\n",
    "    eval_model(model, val_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('inferno')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb978afd226374b4f3169fbe5928f76e34f2c89add9ee986f2aa663d426af832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

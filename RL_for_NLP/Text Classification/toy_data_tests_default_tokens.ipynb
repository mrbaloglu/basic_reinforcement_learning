{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "\n",
    "root_path = \"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\"\n",
    "info_path = \"\"\n",
    "sep = '/'\n",
    "if platform.system() == \"Windows\":\n",
    "    info_path = root_path + \"\\\\NLP_datasets\\\\toy_data\\\\data_info.json\"\n",
    "    sep = '\\\\'\n",
    "    print(\"Running on windows...\")\n",
    "else:\n",
    "    root_path = \"/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning\"\n",
    "    info_path = root_path + \"/NLP_datasets/toy_data/data_info.json\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(root_path)\n",
    "from RL_for_NLP.text_environments import TextEnvClfWithBertTokens, TextEnvClf, TextEnvClfForBertModels\n",
    "from RL_for_NLP.text_reward_functions import calculate_stats_from_cm\n",
    "\n",
    "from RL_for_NLP.text_data_pools import PartialReadingDataPoolWithTokens, PartialReadingDataPoolWithBertTokens\n",
    "import NLP_utils.preprocessing as nlp_processing\n",
    "import reinforce_algorithm_utils as reinforce_utils\n",
    "import policy_networks as pn\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import mlflow\n",
    "from RL_for_NLP.text_reward_functions import calculate_stats_from_cm\n",
    "\n",
    "\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>label_str</th>\n",
       "      <th>review_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75220</td>\n",
       "      <td>emphasis attraction adware paste geography usu...</td>\n",
       "      <td>0</td>\n",
       "      <td>bad</td>\n",
       "      <td>[7501, 26, 8285, 1713, 9550, 5291, 4490, 6625,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48955</td>\n",
       "      <td>cole worcester automation cal mating recommend...</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "      <td>[9235, 9430, 3566, 7007, 9631, 7841, 2612, 972...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44966</td>\n",
       "      <td>functionality later wines think citysearch scr...</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "      <td>[8400, 9089, 6629, 5490, 5871, 807, 6807, 5661...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13568</td>\n",
       "      <td>picked knowing wishlist optional start temp du...</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "      <td>[6444, 4686, 4881, 18, 165, 3181, 7675, 2799, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92727</td>\n",
       "      <td>serves screensaver effectively gale timer teac...</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "      <td>[6630, 1844, 8995, 7676, 7010, 963, 1967, 6264...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review  label  \\\n",
       "0       75220  emphasis attraction adware paste geography usu...      0   \n",
       "1       48955  cole worcester automation cal mating recommend...      1   \n",
       "2       44966  functionality later wines think citysearch scr...      1   \n",
       "3       13568  picked knowing wishlist optional start temp du...      1   \n",
       "4       92727  serves screensaver effectively gale timer teac...      1   \n",
       "\n",
       "  label_str                                   review_tokenized  \n",
       "0       bad  [7501, 26, 8285, 1713, 9550, 5291, 4490, 6625,...  \n",
       "1      good  [9235, 9430, 3566, 7007, 9631, 7841, 2612, 972...  \n",
       "2      good  [8400, 9089, 6629, 5490, 5871, 807, 6807, 5661...  \n",
       "3      good  [6444, 4686, 4881, 18, 165, 3181, 7675, 2799, ...  \n",
       "4      good  [6630, 1844, 8995, 7676, 7010, 963, 1967, 6264...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(info_path, \"r\") as f:\n",
    "    data_info = json.load(f)\n",
    "\n",
    "\n",
    "data_train = nlp_processing.openDfFromPickle(data_info[\"path\"] + sep + \"toy_data/toy-data-train.pkl\")\n",
    "print(data_train.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 10002\n"
     ]
    }
   ],
   "source": [
    "# declare some hyperparameters\n",
    "WINDOW_SIZE = 5\n",
    "MAX_STEPS = int(1e+5)\n",
    "TRAIN_STEPS = int(1e+3)\n",
    "VOCAB_SIZE = data_info[\"vocab_size\"]\n",
    "REWARD_FN = \"score\"\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length in pool: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding the data and populating samples...: 100%|██████████| 80000/80000 [00:03<00:00, 21789.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length in pool: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding the data and populating samples...: 100%|██████████| 10000/10000 [00:00<00:00, 24034.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length in pool: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding the data and populating samples...: 100%|██████████| 10000/10000 [00:00<00:00, 24295.14it/s]\n"
     ]
    }
   ],
   "source": [
    "data_test = nlp_processing.openDfFromPickle(data_info[\"path\"] + sep + \"toy_data/toy-data-test.pkl\")\n",
    "data_val = nlp_processing.openDfFromPickle(data_info[\"path\"] + sep + \"toy_data/toy-data-val.pkl\")\n",
    "\n",
    "train_pool = PartialReadingDataPoolWithTokens(data_train, \"review\", \"label\", WINDOW_SIZE, mask=False)\n",
    "test_pool = PartialReadingDataPoolWithTokens(data_test, \"review\", \"label\", WINDOW_SIZE, mask=False)\n",
    "val_pool = PartialReadingDataPoolWithTokens(data_val, \"review\", \"label\", WINDOW_SIZE, mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All environments are created.\n"
     ]
    }
   ],
   "source": [
    "train_env = TextEnvClf(train_pool, VOCAB_SIZE, MAX_STEPS, \"score\", False)\n",
    "val_env = TextEnvClf(val_pool, VOCAB_SIZE, 2000, \"score\", False)\n",
    "test_env = TextEnvClf(test_pool, VOCAB_SIZE, 2000, \"score\", False)\n",
    "print(\"All environments are created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, env, total_timesteps=10000):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    actions = []\n",
    "    seen_samples = 0\n",
    "    for _ in tqdm(range(total_timesteps)):\n",
    "        action, _states = model.predict(obs)\n",
    "        action = action.item()\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        action = env.action_space.ix_to_action(action)\n",
    "        if action in env.pool.possible_actions:\n",
    "            seen_samples += 1\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "        actions.append(action)\n",
    "        total_reward += rewards\n",
    "    print(\"---------------------------------------------\");\n",
    "    print(f\"Total Steps and seen samples: {len(actions), seen_samples}\")\n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "    print(f\"Stats:  {calculate_stats_from_cm(env.confusion_matrix)}\")\n",
    "    acts = list(Counter(actions).keys())\n",
    "    freqs = list(Counter(actions).values())\n",
    "    total = len(actions)\n",
    "    print(f\"Action stats --  {[{acts[ii]: freqs[ii]/total} for ii in range(len(acts))]}\")\n",
    "    print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1fe7232b6a40cba8e10318c1ebbfdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= On train env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:06<00:00, 1516.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9850)\n",
      "Total reward: 9520.750000000025\n",
      "Stats:  {'accuracy': 0.791981739309166, 'precision': 0.7832945113610537, 'recall': 0.7880666084156995, 'f1': 0.7852384170649513}\n",
      "Action stats --  [{'bad': 0.3963}, {'good': 0.5887}, {'<next>': 0.015}]\n",
      "---------------------------------------------\n",
      "======= On val env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:07<00:00, 1274.39it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58c082d4f2e43fe9db0e26c3210fc46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9829)\n",
      "Total reward: -1257.2900000000013\n",
      "Stats:  {'accuracy': 0.4986265133787771, 'precision': 0.49841337435099087, 'recall': 0.4983476083106628, 'f1': 0.49347745975097457}\n",
      "Action stats --  [{'good': 0.4904}, {'bad': 0.4925}, {'<next>': 0.0171}]\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= On train env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:07<00:00, 1276.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9827)\n",
      "Total reward: 9446.23000000003\n",
      "Stats:  {'accuracy': 0.7999437679780262, 'precision': 0.7917148120808506, 'recall': 0.7981543102766819, 'f1': 0.7940914582031421}\n",
      "Action stats --  [{'bad': 0.3968}, {'good': 0.5859}, {'<next>': 0.0173}]\n",
      "---------------------------------------------\n",
      "======= On val env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:07<00:00, 1360.49it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1aef016d6f490daa9c4c9cb5f92337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9841)\n",
      "Total reward: -1234.1600000000012\n",
      "Stats:  {'accuracy': 0.4991865785460092, 'precision': 0.49936922407965906, 'recall': 0.4993430551650617, 'f1': 0.4942414943913668}\n",
      "Action stats --  [{'good': 0.4949}, {'bad': 0.4892}, {'<next>': 0.0159}]\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=pn.RNNExtractor,\n",
    "    features_extractor_kwargs=dict(vocab_size = VOCAB_SIZE, embed_dim = 5,\n",
    "                 rnn_type = \"gru\", rnn_hidden_size = 2, rnn_hidden_out = 2, rnn_bidirectional = True,\n",
    "                 features_dim = 10, units = 10),\n",
    ")\n",
    "\n",
    "\n",
    "model = DQN(\"MlpPolicy\", train_env, policy_kwargs=policy_kwargs, verbose=1, batch_size=64, gamma=1.)\n",
    "for i in range(int(15)):\n",
    "    model.learn(total_timesteps=int(1e+4)*10, reset_num_timesteps=True, progress_bar=True)\n",
    "    print(\"======= On train env: ===============\")\n",
    "    eval_model(model, train_env)\n",
    "    print(\"======= On val env: ===============\")\n",
    "    eval_model(model, val_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforce Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import reinforce_algorithm_utils as rl_monte_carlo\n",
    "import gym\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "8 4\n"
     ]
    }
   ],
   "source": [
    "print(train_env.current_state_input_id.shape)\n",
    "s_size = train_env.current_state_input_id.shape[0]\n",
    "a_size = len(train_env.action_space)\n",
    "print(s_size, a_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"h_sizes\": [64, 32],\n",
    "    \"n_training_episodes\": 100000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 25,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": None,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "import policy_networks as pn\n",
    "\"\"\"policy = pn.Transformer_Baseline_Policy(data_info[\"vocab_size\"], hyperparameters[\"state_space\"], hyperparameters[\"action_space\"],\n",
    "                                        num_heads=1, num_layers=1)\"\"\"\n",
    "policy = pn.RNN_Baseline_Policy(data_info[\"vocab_size\"], hyperparameters[\"state_space\"], hyperparameters[\"action_space\"])\n",
    "optimizer = Adam(policy.parameters(), lr=hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': RNN_Baseline_Policy(\n",
      "  (embed_enc): Embedding(28996, 5, max_norm=True)\n",
      "  (rnn): GRU(5, 2, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=32, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=4, bias=True)\n",
      "), 'embed_enc': Embedding(28996, 5, max_norm=True), 'rnn': GRU(5, 2, num_layers=2, batch_first=True, bidirectional=True), 'flat': Flatten(start_dim=1, end_dim=-1), 'fc1': Linear(in_features=32, out_features=50, bias=True), 'fc2': Linear(in_features=50, out_features=50, bias=True), 'fc3': Linear(in_features=50, out_features=4, bias=True)}\n"
     ]
    }
   ],
   "source": [
    "params = dict(policy.named_modules())\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "\n",
    "params_s = str(params)\n",
    "mlflow.log_dict(params_s, \"params.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rl_monte_carlo.reinforce_algorithm(train_env, policy,\n",
    "                   optimizer,\n",
    "                   hyperparameters[\"n_training_episodes\"], \n",
    "                   hyperparameters[\"max_t\"],\n",
    "                   hyperparameters[\"gamma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "tensor([[[-0.4974, -0.1800,  0.1256, -0.0997, -2.6214],\n",
      "         [-1.6004,  0.7747,  0.3733,  1.3647,  0.0687],\n",
      "         [-0.7068,  0.6754,  0.6912,  1.7885, -0.4849],\n",
      "         [-1.1904, -0.5578,  1.5177, -0.2507, -0.8633],\n",
      "         [ 1.2573,  0.4593,  1.2733,  0.4671,  0.1200]],\n",
      "\n",
      "        [[-1.1528,  2.0933, -0.6916, -0.2654,  1.5681],\n",
      "         [-1.5847, -0.2593,  0.0507,  0.8566, -0.7230],\n",
      "         [-0.7984, -0.9824, -0.1521,  0.4700, -1.4515],\n",
      "         [ 1.7227,  0.5693,  0.6390, -0.5363, -0.9067],\n",
      "         [ 1.0751, -0.5434,  0.9034, -0.1167,  1.8581]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([2, 5, 5])\n",
      "tensor([[[-1.2247e-01, -8.0512e-02, -3.4734e-02, -2.2318e-03,  7.6149e-04,\n",
      "           1.1641e-01, -1.8003e-02,  5.5952e-02,  1.7178e-02, -4.6703e-02,\n",
      "           2.3925e-03,  1.3036e-01,  1.6704e-01,  1.1473e-01, -7.4383e-02,\n",
      "           5.7412e-02,  8.1825e-02, -4.2972e-02,  9.8029e-03, -1.9270e-01,\n",
      "           5.2298e-02,  1.0932e-01],\n",
      "         [-1.6308e-01, -1.0198e-01, -4.1595e-02, -2.6715e-02,  8.6452e-03,\n",
      "           1.9430e-01, -3.9827e-02,  7.7867e-02,  2.8812e-02, -7.5490e-02,\n",
      "           4.6273e-03,  1.2466e-01,  1.5996e-01,  1.1188e-01, -7.3241e-02,\n",
      "           6.5436e-02,  8.1498e-02, -4.9271e-02,  4.6259e-03, -1.9625e-01,\n",
      "           4.8194e-02,  9.5234e-02],\n",
      "         [-1.7284e-01, -1.0413e-01, -3.4377e-02, -4.6000e-02,  1.9985e-02,\n",
      "           2.4257e-01, -6.1291e-02,  8.8687e-02,  3.5620e-02, -8.8313e-02,\n",
      "           4.0086e-03,  1.1054e-01,  1.4540e-01,  9.9597e-02, -6.4652e-02,\n",
      "           6.8844e-02,  7.6686e-02, -5.4924e-02, -2.1404e-04, -1.8868e-01,\n",
      "           4.4651e-02,  7.9386e-02],\n",
      "         [-1.7069e-01, -1.0043e-01, -2.0737e-02, -5.7775e-02,  3.1465e-02,\n",
      "           2.7287e-01, -8.1545e-02,  9.5495e-02,  3.9208e-02, -9.0876e-02,\n",
      "           3.3776e-03,  8.5820e-02,  1.1718e-01,  7.6870e-02, -4.7925e-02,\n",
      "           6.6348e-02,  6.4312e-02, -5.8368e-02, -4.1747e-03, -1.6452e-01,\n",
      "           3.7354e-02,  6.0442e-02],\n",
      "         [-1.6364e-01, -9.5599e-02, -3.5386e-03, -6.7247e-02,  4.0360e-02,\n",
      "           2.9116e-01, -1.0003e-01,  1.0215e-01,  4.1441e-02, -8.6942e-02,\n",
      "           7.7907e-03,  4.8998e-02,  6.9198e-02,  4.2961e-02, -2.3435e-02,\n",
      "           5.1084e-02,  3.9808e-02, -5.0649e-02, -5.5412e-03, -1.1106e-01,\n",
      "           2.3310e-02,  3.6188e-02]],\n",
      "\n",
      "        [[-1.2248e-01, -8.0515e-02, -3.4633e-02, -2.2718e-03,  7.6362e-04,\n",
      "           1.1647e-01, -1.8002e-02,  5.6003e-02,  1.7107e-02, -4.6681e-02,\n",
      "           2.4068e-03,  1.3049e-01,  1.6695e-01,  1.1468e-01, -7.4388e-02,\n",
      "           5.7498e-02,  8.2024e-02, -4.2844e-02,  9.9580e-03, -1.9292e-01,\n",
      "           5.2392e-02,  1.0932e-01],\n",
      "         [-1.6308e-01, -1.0198e-01, -4.1431e-02, -2.6787e-02,  8.6561e-03,\n",
      "           1.9438e-01, -3.9813e-02,  7.7958e-02,  2.8707e-02, -7.5483e-02,\n",
      "           4.6387e-03,  1.2476e-01,  1.5985e-01,  1.1181e-01, -7.3214e-02,\n",
      "           6.5521e-02,  8.1696e-02, -4.9126e-02,  4.7533e-03, -1.9648e-01,\n",
      "           4.8325e-02,  9.5297e-02],\n",
      "         [-1.7283e-01, -1.0411e-01, -3.4194e-02, -4.6080e-02,  1.9999e-02,\n",
      "           2.4264e-01, -6.1267e-02,  8.8812e-02,  3.5499e-02, -8.8330e-02,\n",
      "           3.9985e-03,  1.1060e-01,  1.4527e-01,  9.9519e-02, -6.4596e-02,\n",
      "           6.8912e-02,  7.6854e-02, -5.4793e-02, -1.1568e-04, -1.8888e-01,\n",
      "           4.4799e-02,  7.9516e-02],\n",
      "         [-1.7068e-01, -1.0038e-01, -2.0575e-02, -5.7843e-02,  3.1472e-02,\n",
      "           2.7293e-01, -8.1512e-02,  9.5638e-02,  3.9088e-02, -9.0917e-02,\n",
      "           3.3340e-03,  8.5839e-02,  1.1705e-01,  7.6798e-02, -4.7854e-02,\n",
      "           6.6389e-02,  6.4421e-02, -5.8280e-02, -4.1052e-03, -1.6465e-01,\n",
      "           3.7494e-02,  6.0613e-02],\n",
      "         [-1.6364e-01, -9.5526e-02, -3.4258e-03, -6.7300e-02,  4.0343e-02,\n",
      "           2.9119e-01, -9.9978e-02,  1.0229e-01,  4.1340e-02, -8.7001e-02,\n",
      "           7.7225e-03,  4.8995e-02,  6.9113e-02,  4.2921e-02, -2.3376e-02,\n",
      "           5.1100e-02,  3.9847e-02, -5.0618e-02, -5.5003e-03, -1.1112e-01,\n",
      "           2.3411e-02,  3.6332e-02]]], grad_fn=<TransposeBackward0>)\n",
      "torch.Size([2, 5, 22]) torch.Size([14, 2, 11])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.Tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]]).int()\n",
    "emb = nn.Embedding(20, 5)\n",
    "lstm = nn.LSTM(5, 11, 7, bidirectional=True, batch_first=True)\n",
    "print(x.shape)\n",
    "x = emb(x)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x, (h, _) = lstm(x)\n",
    "print(x)\n",
    "print(x.shape, h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8931568569324175"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1e-1*(np.log2(500000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_agent(train_env, 10, 100, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_agent(val_env, 10, 100, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_on_clf(val_env, policy, pos_label=\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, env):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    actions = []\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        action = action.item()\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        actions.append(env.action_space.ix_to_action(action))\n",
    "        total_reward += rewards\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(f\"Predicted Label {actions}\")\n",
    "    print(f\"Oracle Label: {env.current_label}\")\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "    print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(policy, val_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow.set_tracking_uri(\"file:/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning/agent_models/rt-polarity\")\n",
    "mlflow.set_experiment(\"rnn\")\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"n_episodes\", 500)\n",
    "mlflow.pytorch.log_model(policy, \"rnn-default\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning/agent_models/rt-polarity\")\n",
    "uri = \"runs:/9f09e3fa6df6427ba2889e89ac787bcc/rnn-default\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_agent(env, 10, 100, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_on_clf(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(policy=MlpPolicy, env=train_env, learning_rate=0.001, batch_size=3)\n",
    "\n",
    "for i in range(int(5)):\n",
    "    model.learn(total_timesteps=int(1e+3), reset_num_timesteps=False)\n",
    "    eval_model(model, val_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the state space and action space\n",
    "s_size = train_env.observation_space.shape[0]\n",
    "a_size = len(train_env.action_space.actions)\n",
    "print(s_size, a_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(policy = \"MlpPolicy\",\n",
    "            env = train_env,\n",
    "            gae_lambda = 0.9,\n",
    "            gamma = 0.99,\n",
    "            learning_rate = 0.00096,\n",
    "            max_grad_norm = 0.5,\n",
    "            n_steps = 8,\n",
    "            vf_coef = 0.4,\n",
    "            ent_coef = 0.0,\n",
    "            tensorboard_log = \"./tensorboard\",\n",
    "            policy_kwargs=dict(\n",
    "            log_std_init=-2, ortho_init=False),\n",
    "            normalize_advantage=False,\n",
    "            use_rms_prop= True,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    model.learn(500, log_interval=250)\n",
    "    print(evaluate_policy(model, val_env, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, val_env, n_eval_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_on_clf(val_env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('inferno')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb978afd226374b4f3169fbe5928f76e34f2c89add9ee986f2aa663d426af832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

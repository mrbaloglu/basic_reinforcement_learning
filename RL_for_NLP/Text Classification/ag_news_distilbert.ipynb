{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning\")\n",
    "import os\n",
    "os.chdir(\"/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning\")\n",
    "\n",
    "import NLP_utils.preprocessing as nlp_processing\n",
    "from RL_for_NLP.text_environments import TextEnvClfWithBertTokens, TextEnvClf, TextEnvClfForBertModels\n",
    "from RL_for_NLP.text_reward_functions import calculate_stats_from_cm\n",
    "\n",
    "from RL_for_NLP.text_data_pools import PartialReadingDataPoolWithTokens, PartialReadingDataPoolWithBertTokens\n",
    "import reinforce_algorithm_utils as rl_monte_carlo\n",
    "import policy_networks as pn\n",
    "\n",
    "import torch as th\n",
    "from torch.optim import Adam\n",
    "import mlflow\n",
    "from RL_for_NLP.text_reward_functions import calculate_stats_from_cm\n",
    "\n",
    "import actor_critic_algortihm_utils as a2c_utils \n",
    "\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length in pool: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding the data and populating samples...: 100%|██████████| 96000/96000 [00:08<00:00, 10835.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Inside constructor ---------\n",
      "Input id shape/type:  (50,) <class 'numpy.ndarray'>\n",
      "Attn mask shape/type:  (50,) <class 'numpy.ndarray'>\n",
      "------------- End Constructor-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = nlp_processing.openDfFromPickle(\"NLP_datasets/ag_news/ag_news_train_distilbert-base-uncased.pkl\")\n",
    "pool = PartialReadingDataPoolWithBertTokens(data, \"text\", \"label\", 512, 50, mask = True)\n",
    "env = TextEnvClfForBertModels(pool, 30522, int(1e+5), \"score\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_id': array([  101, 10556, 24175,  2072, 13980,  2602,  8320,  2000, 21250,\n",
       "         7041,  9455,  2343, 24811, 10556, 24175,  2072,  2038,  2187,\n",
       "         2010,  4600, 13313,  7328,  1999, 21073,  2005,  2010,  2034,\n",
       "         2602,  8320,  1999,  1996,  2197,  2733,  1997, 18524,  2005,\n",
       "         2023,  5095,  2034,  2412,  3622,  3864,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0]),\n",
       " 'attn_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_id': array([  101,  3027,  3366,  4978,  2733,  2659,  2327,  6661,  2031,\n",
       "         5357,  2000,  2093,  2733,  2659,  2044,  2019,  2220,  5376,\n",
       "         2006,  1996,  2067,  1997,  3893,  5971,  2739,  3478,  2000,\n",
       "         8587, 11071,  1998,  6428,  2084,  3517,  7027,  4341,  2951,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]),\n",
       " 'attn_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2 = env.current_state\n",
    "inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.tensor(inputs2[\"input_id\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = distilbert(th.tensor(env.current_state[\"input_id\"]).unsqueeze(0), th.tensor(env.current_state[\"attn_mask\"]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.0948, -0.0587,  0.1873,  ..., -0.2871,  0.5707,  0.1970],\n",
       "         [ 0.2357, -0.1329,  0.6317,  ...,  0.3515,  0.5148, -0.2955],\n",
       "         [-0.2966, -0.3269,  0.6578,  ..., -0.0205,  0.4010, -0.0548],\n",
       "         ...,\n",
       "         [ 0.1536,  0.0169,  0.3824,  ..., -0.0989,  0.1536, -0.0054],\n",
       "         [ 0.0532,  0.1062,  0.2115,  ..., -0.0597,  0.1105, -0.0880],\n",
       "         [ 0.2399, -0.1958,  0.3171,  ..., -0.0178,  0.1243,  0.0880]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_module() missing 1 required positional argument: 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9y/0_p75b4j0dvfw0qn8l2qvyfh0000gn/T/ipykernel_64737/14078804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistilbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdistilbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: add_module() missing 1 required positional argument: 'module'"
     ]
    }
   ],
   "source": [
    "distilbert.add_module(th.nn.Flatten())\n",
    "distilbert.add_module(th.nn.Linear(50*768, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = a2c_utils.DistibertActorCriticPolicy(50, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2148, 0.2861, 0.2463, 0.2528]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.1418]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(th.tensor(env.current_state[\"input_id\"]).unsqueeze(0), th.tensor(env.current_state[\"attn_mask\"]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('inferno')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb978afd226374b4f3169fbe5928f76e34f2c89add9ee986f2aa663d426af832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

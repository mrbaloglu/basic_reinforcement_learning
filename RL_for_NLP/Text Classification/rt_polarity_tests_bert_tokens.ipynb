{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "\n",
    "root_path = \"C:\\\\Users\\\\mrbal\\\\Documents\\\\NLP\\\\RL\\\\basic_reinforcement_learning\"\n",
    "info_path = \"\"\n",
    "sep = '/'\n",
    "if platform.system() == \"Windows\":\n",
    "    info_path = root_path + \"\\\\NLP_datasets\\\\RT_Polarity\\\\data_info_bert_windows.json\"\n",
    "    sep = '\\\\'\n",
    "    print(\"Running on windows...\")\n",
    "else:\n",
    "    root_path = \"/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning\"\n",
    "    info_path = root_path + \"/NLP_datasets/RT_Polarity/data_info_bert_macos.json\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(root_path)\n",
    "from RL_for_NLP.text_environments import TextEnvClfWithBertTokens, TextEnvClf, TextEnvClfForBertModels\n",
    "from RL_for_NLP.text_reward_functions import calculate_stats_from_cm\n",
    "\n",
    "from RL_for_NLP.text_data_pools import PartialReadingDataPoolWithTokens, PartialReadingDataPoolWithBertTokens\n",
    "import NLP_utils.preprocessing as nlp_processing\n",
    "import reinforce_algorithm_utils as rl_monte_carlo\n",
    "import policy_networks as pn\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import mlflow\n",
    "from RL_for_NLP.text_reward_functions import calculate_stats_from_cm\n",
    "\n",
    "\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8528, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>label_str</th>\n",
       "      <th>review_bert_input_ids</th>\n",
       "      <th>review_bert_token_type_ids</th>\n",
       "      <th>review_bert_attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5829</td>\n",
       "      <td>1</td>\n",
       "      <td>does what fine documentary does best it extend...</td>\n",
       "      <td>good</td>\n",
       "      <td>[101, 1674, 1184, 2503, 4148, 1674, 1436, 1122...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3209</td>\n",
       "      <td>0</td>\n",
       "      <td>it drowns in sap</td>\n",
       "      <td>bad</td>\n",
       "      <td>[101, 1122, 22592, 1116, 1107, 21718, 1643, 10...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4323</td>\n",
       "      <td>0</td>\n",
       "      <td>the scriptwriters are no less menace to societ...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[101, 1103, 5444, 13814, 1116, 1132, 1185, 175...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281</td>\n",
       "      <td>0</td>\n",
       "      <td>there an excellent minute film here unfortunat...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[101, 1175, 1126, 6548, 2517, 1273, 1303, 1667...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>narratively trouble every day is plodding mess</td>\n",
       "      <td>bad</td>\n",
       "      <td>[101, 8195, 1193, 3819, 1451, 1285, 1110, 185,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                             review  \\\n",
       "0        5829      1  does what fine documentary does best it extend...   \n",
       "1        3209      0                                  it drowns in sap    \n",
       "2        4323      0  the scriptwriters are no less menace to societ...   \n",
       "3         281      0  there an excellent minute film here unfortunat...   \n",
       "4         100      0    narratively trouble every day is plodding mess    \n",
       "\n",
       "  label_str                              review_bert_input_ids  \\\n",
       "0      good  [101, 1674, 1184, 2503, 4148, 1674, 1436, 1122...   \n",
       "1       bad  [101, 1122, 22592, 1116, 1107, 21718, 1643, 10...   \n",
       "2       bad  [101, 1103, 5444, 13814, 1116, 1132, 1185, 175...   \n",
       "3       bad  [101, 1175, 1126, 6548, 2517, 1273, 1303, 1667...   \n",
       "4       bad  [101, 8195, 1193, 3819, 1451, 1285, 1110, 185,...   \n",
       "\n",
       "                          review_bert_token_type_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                          review_bert_attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(info_path, \"r\") as f:\n",
    "    data_info = json.load(f)\n",
    "\n",
    "\n",
    "data_train = nlp_processing.openDfFromPickle(data_info[\"path\"] + sep + \"rt-polarity-train-bert.pkl\")\n",
    "print(data_train.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 28996\n"
     ]
    }
   ],
   "source": [
    "# declare some hyperparameters\n",
    "WINDOW_SIZE = 16\n",
    "MAX_STEPS = int(1e+5)\n",
    "TRAIN_STEPS = int(1e+3)\n",
    "VOCAB_SIZE = data_info[\"vocab_size\"]\n",
    "REWARD_FN = \"score\"\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length in pool: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding the data and populating samples...: 100%|██████████| 8528/8528 [00:00<00:00, 26297.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length in pool: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding the data and populating samples...: 100%|██████████| 1067/1067 [00:00<00:00, 30675.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length in pool: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding the data and populating samples...: 100%|██████████| 1066/1066 [00:00<00:00, 26552.53it/s]\n"
     ]
    }
   ],
   "source": [
    "data_test = nlp_processing.openDfFromPickle(data_info[\"path\"] + sep + \"rt-polarity-test-bert.pkl\")\n",
    "data_val = nlp_processing.openDfFromPickle(data_info[\"path\"] + sep + \"rt-polarity-val-bert.pkl\")\n",
    "\n",
    "train_pool = PartialReadingDataPoolWithBertTokens(data_train, \"review\", \"label\", WINDOW_SIZE, mask=False)\n",
    "test_pool = PartialReadingDataPoolWithBertTokens(data_test, \"review\", \"label\", WINDOW_SIZE, mask=False)\n",
    "val_pool = PartialReadingDataPoolWithBertTokens(data_val, \"review\", \"label\", WINDOW_SIZE, mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Inside constructor ---------\n",
      "State shape/type:  (16,) <class 'numpy.ndarray'>\n",
      "------------- End Constructor-------------\n",
      "---- Inside constructor ---------\n",
      "State shape/type:  (16,) <class 'numpy.ndarray'>\n",
      "------------- End Constructor-------------\n",
      "---- Inside constructor ---------\n",
      "State shape/type:  (16,) <class 'numpy.ndarray'>\n",
      "------------- End Constructor-------------\n",
      "All environments are created.\n"
     ]
    }
   ],
   "source": [
    "train_env = TextEnvClfWithBertTokens(train_pool, VOCAB_SIZE, MAX_STEPS, \"score\", False)\n",
    "val_env = TextEnvClfWithBertTokens(val_pool, VOCAB_SIZE, 2000, \"score\", False)\n",
    "test_env = TextEnvClfWithBertTokens(test_pool, VOCAB_SIZE, 2000, \"score\", False)\n",
    "print(\"All environments are created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, env, total_timesteps=10000):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    actions = []\n",
    "    seen_samples = 0\n",
    "    for _ in tqdm(range(total_timesteps)):\n",
    "        action, _states = model.predict(obs)\n",
    "        action = action.item()\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        action = env.action_space.ix_to_action(action)\n",
    "        if action in env.pool.possible_actions:\n",
    "            seen_samples += 1\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "        actions.append(action)\n",
    "        total_reward += rewards\n",
    "    print(\"---------------------------------------------\");\n",
    "    print(f\"Total Steps and seen samples: {len(actions), seen_samples}\")\n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "    print(f\"Stats:  {calculate_stats_from_cm(env.confusion_matrix)}\")\n",
    "    acts = list(Counter(actions).keys())\n",
    "    freqs = list(Counter(actions).values())\n",
    "    total = len(actions)\n",
    "    print(f\"Action stats --  {[{acts[ii]: freqs[ii]/total} for ii in range(len(acts))]}\")\n",
    "    print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90883ce88ca344e0b065d6f9fd674458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= On train env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:12<00:00, 799.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9797)\n",
      "Total reward: 9427.780000000035\n",
      "Stats:  {'accuracy': 0.8643023395660449, 'precision': 0.8147768538162334, 'recall': 0.8595807517723904, 'f1': 0.8318673587498024}\n",
      "Action stats --  [{'bad': 0.2505}, {'good': 0.7292}, {'<next>': 0.0203}]\n",
      "---------------------------------------------\n",
      "======= On val env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:12<00:00, 797.96it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3b96bb591f4692a82b7dde1a8e7c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9847)\n",
      "Total reward: -6421.2199999999775\n",
      "Stats:  {'accuracy': 0.2656646694424698, 'precision': 0.5387543053452688, 'recall': 0.5048133282659195, 'f1': 0.22604236928654717}\n",
      "Action stats --  [{'<next>': 0.0153}, {'good': 0.9612}, {'bad': 0.0235}]\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= On train env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:12<00:00, 787.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9831)\n",
      "Total reward: 9461.44000000003\n",
      "Stats:  {'accuracy': 0.8671904571551879, 'precision': 0.8181857201800309, 'recall': 0.8645070226408079, 'f1': 0.8357334645761545}\n",
      "Action stats --  [{'bad': 0.2539}, {'<next>': 0.0169}, {'good': 0.7292}]\n",
      "---------------------------------------------\n",
      "======= On val env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:13<00:00, 750.13it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afaefb63d0a84883975bc5ae9139d05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9839)\n",
      "Total reward: -6354.889999999975\n",
      "Stats:  {'accuracy': 0.2670425683226659, 'precision': 0.5500816671201012, 'recall': 0.5063388515598319, 'f1': 0.22766639567809033}\n",
      "Action stats --  [{'good': 0.9595}, {'bad': 0.0244}, {'<next>': 0.0161}]\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= On train env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:14<00:00, 699.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9837)\n",
      "Total reward: 9512.380000000026\n",
      "Stats:  {'accuracy': 0.8679525117600594, 'precision': 0.8190957843023632, 'recall': 0.8661102941351435, 'f1': 0.8368317196580727}\n",
      "Action stats --  [{'bad': 0.2541}, {'good': 0.7296}, {'<next>': 0.0163}]\n",
      "---------------------------------------------\n",
      "======= On val env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:12<00:00, 769.59it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0116b6b44ad342e99130bef8d213c7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9852)\n",
      "Total reward: -6434.269999999977\n",
      "Stats:  {'accuracy': 0.2664364547362719, 'precision': 0.5424357559276052, 'recall': 0.5054019315202667, 'f1': 0.22707168994445953}\n",
      "Action stats --  [{'good': 0.9608}, {'bad': 0.0244}, {'<next>': 0.0148}]\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= On train env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [06:56<00:00, 24.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9824)\n",
      "Total reward: 9432.010000000028\n",
      "Stats:  {'accuracy': 0.8683828912790512, 'precision': 0.8196060626037946, 'recall': 0.8669466118071356, 'f1': 0.8374323215664901}\n",
      "Action stats --  [{'bad': 0.2551}, {'good': 0.7273}, {'<next>': 0.0176}]\n",
      "---------------------------------------------\n",
      "======= On val env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [09:02<00:00, 18.45it/s] "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4df0fe7a9641f0a685a67765ec5560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9821)\n",
      "Total reward: -6399.7099999999755\n",
      "Stats:  {'accuracy': 0.26629233466297414, 'precision': 0.538344045754637, 'recall': 0.5049686097531108, 'f1': 0.22710042212296983}\n",
      "Action stats --  [{'good': 0.9563}, {'<next>': 0.0179}, {'bad': 0.0258}]\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= On train env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:14<00:00, 685.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9834)\n",
      "Total reward: 9495.91000000003\n",
      "Stats:  {'accuracy': 0.8686360557157585, 'precision': 0.8199084868959574, 'recall': 0.8674882291452275, 'f1': 0.8377985734682387}\n",
      "Action stats --  [{'bad': 0.2548}, {'good': 0.7286}, {'<next>': 0.0166}]\n",
      "---------------------------------------------\n",
      "======= On val env: ===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:13<00:00, 738.64it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e2eaedddcb4cb8ba682159641693fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Total Steps and seen samples: (10000, 9817)\n",
      "Total reward: -6342.919999999974\n",
      "Stats:  {'accuracy': 0.2666951358386205, 'precision': 0.5410722457690277, 'recall': 0.5053613428259242, 'f1': 0.22758900172688548}\n",
      "Action stats --  [{'good': 0.9563}, {'bad': 0.0254}, {'<next>': 0.0183}]\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9y/0_p75b4j0dvfw0qn8l2qvyfh0000gn/T/ipykernel_60629/1267415622.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MlpPolicy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e+4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"======= On train env: ===============\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/inferno/lib/python3.7/site-packages/stable_baselines3/dqn/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         )\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/inferno/lib/python3.7/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0;31m# Special case when the user passes `gradient_steps=0`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/inferno/lib/python3.7/site-packages/stable_baselines3/dqn/dqn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;31m# Get current Q-values estimates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0mcurrent_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# Retrieve the q-values for the actions from the replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/inferno/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/inferno/lib/python3.7/site-packages/stable_baselines3/dqn/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mestimated\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mValue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/inferno/lib/python3.7/site-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_extractor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No features extractor was set\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mpreprocessed_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_constructor_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/inferno/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RL/basic_reinforcement_learning/policy_networks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mx_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mx_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0mx_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/inferno/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/inferno/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 943\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    944\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=pn.RNNExtractor,\n",
    "    features_extractor_kwargs=dict(vocab_size = VOCAB_SIZE, embed_dim = 5,\n",
    "                 rnn_type = \"gru\", rnn_hidden_size = 2, rnn_hidden_out = 2, rnn_bidirectional = True,\n",
    "                 features_dim = 10, units = 10),\n",
    ")\n",
    "\n",
    "\n",
    "model = DQN(\"MlpPolicy\", train_env, policy_kwargs=policy_kwargs, verbose=1, batch_size=64, gamma=1.)\n",
    "for i in range(int(15)):\n",
    "    model.learn(total_timesteps=int(1e+4)*15, reset_num_timesteps=True, progress_bar=True)\n",
    "    print(\"======= On train env: ===============\")\n",
    "    eval_model(model, train_env)\n",
    "    print(\"======= On val env: ===============\")\n",
    "    eval_model(model, val_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforce Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import reinforce_algorithm_utils as rl_monte_carlo\n",
    "import gym\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "8 4\n"
     ]
    }
   ],
   "source": [
    "print(train_env.current_state_input_id.shape)\n",
    "s_size = train_env.current_state_input_id.shape[0]\n",
    "a_size = len(train_env.action_space)\n",
    "print(s_size, a_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"h_sizes\": [64, 32],\n",
    "    \"n_training_episodes\": 100000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 25,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": None,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "import policy_networks as pn\n",
    "\"\"\"policy = pn.Transformer_Baseline_Policy(data_info[\"vocab_size\"], hyperparameters[\"state_space\"], hyperparameters[\"action_space\"],\n",
    "                                        num_heads=1, num_layers=1)\"\"\"\n",
    "policy = pn.RNN_Baseline_Policy(data_info[\"vocab_size\"], hyperparameters[\"state_space\"], hyperparameters[\"action_space\"])\n",
    "optimizer = Adam(policy.parameters(), lr=hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': RNN_Baseline_Policy(\n",
      "  (embed_enc): Embedding(28996, 5, max_norm=True)\n",
      "  (rnn): GRU(5, 2, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=32, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=4, bias=True)\n",
      "), 'embed_enc': Embedding(28996, 5, max_norm=True), 'rnn': GRU(5, 2, num_layers=2, batch_first=True, bidirectional=True), 'flat': Flatten(start_dim=1, end_dim=-1), 'fc1': Linear(in_features=32, out_features=50, bias=True), 'fc2': Linear(in_features=50, out_features=50, bias=True), 'fc3': Linear(in_features=50, out_features=4, bias=True)}\n"
     ]
    }
   ],
   "source": [
    "params = dict(policy.named_modules())\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "\n",
    "params_s = str(params)\n",
    "mlflow.log_dict(params_s, \"params.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rl_monte_carlo.reinforce_algorithm(train_env, policy,\n",
    "                   optimizer,\n",
    "                   hyperparameters[\"n_training_episodes\"], \n",
    "                   hyperparameters[\"max_t\"],\n",
    "                   hyperparameters[\"gamma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "tensor([[[-0.4974, -0.1800,  0.1256, -0.0997, -2.6214],\n",
      "         [-1.6004,  0.7747,  0.3733,  1.3647,  0.0687],\n",
      "         [-0.7068,  0.6754,  0.6912,  1.7885, -0.4849],\n",
      "         [-1.1904, -0.5578,  1.5177, -0.2507, -0.8633],\n",
      "         [ 1.2573,  0.4593,  1.2733,  0.4671,  0.1200]],\n",
      "\n",
      "        [[-1.1528,  2.0933, -0.6916, -0.2654,  1.5681],\n",
      "         [-1.5847, -0.2593,  0.0507,  0.8566, -0.7230],\n",
      "         [-0.7984, -0.9824, -0.1521,  0.4700, -1.4515],\n",
      "         [ 1.7227,  0.5693,  0.6390, -0.5363, -0.9067],\n",
      "         [ 1.0751, -0.5434,  0.9034, -0.1167,  1.8581]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([2, 5, 5])\n",
      "tensor([[[-1.2247e-01, -8.0512e-02, -3.4734e-02, -2.2318e-03,  7.6149e-04,\n",
      "           1.1641e-01, -1.8003e-02,  5.5952e-02,  1.7178e-02, -4.6703e-02,\n",
      "           2.3925e-03,  1.3036e-01,  1.6704e-01,  1.1473e-01, -7.4383e-02,\n",
      "           5.7412e-02,  8.1825e-02, -4.2972e-02,  9.8029e-03, -1.9270e-01,\n",
      "           5.2298e-02,  1.0932e-01],\n",
      "         [-1.6308e-01, -1.0198e-01, -4.1595e-02, -2.6715e-02,  8.6452e-03,\n",
      "           1.9430e-01, -3.9827e-02,  7.7867e-02,  2.8812e-02, -7.5490e-02,\n",
      "           4.6273e-03,  1.2466e-01,  1.5996e-01,  1.1188e-01, -7.3241e-02,\n",
      "           6.5436e-02,  8.1498e-02, -4.9271e-02,  4.6259e-03, -1.9625e-01,\n",
      "           4.8194e-02,  9.5234e-02],\n",
      "         [-1.7284e-01, -1.0413e-01, -3.4377e-02, -4.6000e-02,  1.9985e-02,\n",
      "           2.4257e-01, -6.1291e-02,  8.8687e-02,  3.5620e-02, -8.8313e-02,\n",
      "           4.0086e-03,  1.1054e-01,  1.4540e-01,  9.9597e-02, -6.4652e-02,\n",
      "           6.8844e-02,  7.6686e-02, -5.4924e-02, -2.1404e-04, -1.8868e-01,\n",
      "           4.4651e-02,  7.9386e-02],\n",
      "         [-1.7069e-01, -1.0043e-01, -2.0737e-02, -5.7775e-02,  3.1465e-02,\n",
      "           2.7287e-01, -8.1545e-02,  9.5495e-02,  3.9208e-02, -9.0876e-02,\n",
      "           3.3776e-03,  8.5820e-02,  1.1718e-01,  7.6870e-02, -4.7925e-02,\n",
      "           6.6348e-02,  6.4312e-02, -5.8368e-02, -4.1747e-03, -1.6452e-01,\n",
      "           3.7354e-02,  6.0442e-02],\n",
      "         [-1.6364e-01, -9.5599e-02, -3.5386e-03, -6.7247e-02,  4.0360e-02,\n",
      "           2.9116e-01, -1.0003e-01,  1.0215e-01,  4.1441e-02, -8.6942e-02,\n",
      "           7.7907e-03,  4.8998e-02,  6.9198e-02,  4.2961e-02, -2.3435e-02,\n",
      "           5.1084e-02,  3.9808e-02, -5.0649e-02, -5.5412e-03, -1.1106e-01,\n",
      "           2.3310e-02,  3.6188e-02]],\n",
      "\n",
      "        [[-1.2248e-01, -8.0515e-02, -3.4633e-02, -2.2718e-03,  7.6362e-04,\n",
      "           1.1647e-01, -1.8002e-02,  5.6003e-02,  1.7107e-02, -4.6681e-02,\n",
      "           2.4068e-03,  1.3049e-01,  1.6695e-01,  1.1468e-01, -7.4388e-02,\n",
      "           5.7498e-02,  8.2024e-02, -4.2844e-02,  9.9580e-03, -1.9292e-01,\n",
      "           5.2392e-02,  1.0932e-01],\n",
      "         [-1.6308e-01, -1.0198e-01, -4.1431e-02, -2.6787e-02,  8.6561e-03,\n",
      "           1.9438e-01, -3.9813e-02,  7.7958e-02,  2.8707e-02, -7.5483e-02,\n",
      "           4.6387e-03,  1.2476e-01,  1.5985e-01,  1.1181e-01, -7.3214e-02,\n",
      "           6.5521e-02,  8.1696e-02, -4.9126e-02,  4.7533e-03, -1.9648e-01,\n",
      "           4.8325e-02,  9.5297e-02],\n",
      "         [-1.7283e-01, -1.0411e-01, -3.4194e-02, -4.6080e-02,  1.9999e-02,\n",
      "           2.4264e-01, -6.1267e-02,  8.8812e-02,  3.5499e-02, -8.8330e-02,\n",
      "           3.9985e-03,  1.1060e-01,  1.4527e-01,  9.9519e-02, -6.4596e-02,\n",
      "           6.8912e-02,  7.6854e-02, -5.4793e-02, -1.1568e-04, -1.8888e-01,\n",
      "           4.4799e-02,  7.9516e-02],\n",
      "         [-1.7068e-01, -1.0038e-01, -2.0575e-02, -5.7843e-02,  3.1472e-02,\n",
      "           2.7293e-01, -8.1512e-02,  9.5638e-02,  3.9088e-02, -9.0917e-02,\n",
      "           3.3340e-03,  8.5839e-02,  1.1705e-01,  7.6798e-02, -4.7854e-02,\n",
      "           6.6389e-02,  6.4421e-02, -5.8280e-02, -4.1052e-03, -1.6465e-01,\n",
      "           3.7494e-02,  6.0613e-02],\n",
      "         [-1.6364e-01, -9.5526e-02, -3.4258e-03, -6.7300e-02,  4.0343e-02,\n",
      "           2.9119e-01, -9.9978e-02,  1.0229e-01,  4.1340e-02, -8.7001e-02,\n",
      "           7.7225e-03,  4.8995e-02,  6.9113e-02,  4.2921e-02, -2.3376e-02,\n",
      "           5.1100e-02,  3.9847e-02, -5.0618e-02, -5.5003e-03, -1.1112e-01,\n",
      "           2.3411e-02,  3.6332e-02]]], grad_fn=<TransposeBackward0>)\n",
      "torch.Size([2, 5, 22]) torch.Size([14, 2, 11])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.Tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]]).int()\n",
    "emb = nn.Embedding(20, 5)\n",
    "lstm = nn.LSTM(5, 11, 7, bidirectional=True, batch_first=True)\n",
    "print(x.shape)\n",
    "x = emb(x)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "x, (h, _) = lstm(x)\n",
    "print(x)\n",
    "print(x.shape, h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8931568569324175"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1e-1*(np.log2(500000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_agent(train_env, 10, 100, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_agent(val_env, 10, 100, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_on_clf(val_env, policy, pos_label=\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, env):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "    actions = []\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        action = action.item()\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        actions.append(env.action_space.ix_to_action(action))\n",
    "        total_reward += rewards\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(f\"Predicted Label {actions}\")\n",
    "    print(f\"Oracle Label: {env.current_label}\")\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "    print(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(policy, val_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow.set_tracking_uri(\"file:/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning/agent_models/rt-polarity\")\n",
    "mlflow.set_experiment(\"rnn\")\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"n_episodes\", 500)\n",
    "mlflow.pytorch.log_model(policy, \"rnn-default\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:/Users/emrebaloglu/Documents/RL/basic_reinforcement_learning/agent_models/rt-polarity\")\n",
    "uri = \"runs:/9f09e3fa6df6427ba2889e89ac787bcc/rnn-default\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_agent(env, 10, 100, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_on_clf(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(policy=MlpPolicy, env=train_env, learning_rate=0.001, batch_size=3)\n",
    "\n",
    "for i in range(int(5)):\n",
    "    model.learn(total_timesteps=int(1e+3), reset_num_timesteps=False)\n",
    "    eval_model(model, val_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the state space and action space\n",
    "s_size = train_env.observation_space.shape[0]\n",
    "a_size = len(train_env.action_space.actions)\n",
    "print(s_size, a_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(policy = \"MlpPolicy\",\n",
    "            env = train_env,\n",
    "            gae_lambda = 0.9,\n",
    "            gamma = 0.99,\n",
    "            learning_rate = 0.00096,\n",
    "            max_grad_norm = 0.5,\n",
    "            n_steps = 8,\n",
    "            vf_coef = 0.4,\n",
    "            ent_coef = 0.0,\n",
    "            tensorboard_log = \"./tensorboard\",\n",
    "            policy_kwargs=dict(\n",
    "            log_std_init=-2, ortho_init=False),\n",
    "            normalize_advantage=False,\n",
    "            use_rms_prop= True,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    model.learn(500, log_interval=250)\n",
    "    print(evaluate_policy(model, val_env, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, val_env, n_eval_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_monte_carlo.evaluate_on_clf(val_env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('inferno')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb978afd226374b4f3169fbe5928f76e34f2c89add9ee986f2aa663d426af832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
